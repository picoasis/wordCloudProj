我们经常需要对分析的数据提取常用词，做词云展示。比如一些互联网公司会抓取用户的画像，或者每日讨论话题的关键词，形成词云并进行展示。再或者，假如你喜欢某个歌手，想了解这个歌手创作的歌曲中经常用到哪些词语，词云就是个很好的工具。最后，只需要将词云生成一张图片就可以直观地看到结果。

那么在今天的实战项目里，有 3 个目标需要掌握：

1. 掌握词云分析工具，并进行可视化呈现；
2. 掌握 Python 爬虫，对网页的数据进行爬取；
3. 掌握 XPath 工具，分析提取想要的元素 。



[TOC]

# 基本概念

1.语 料 库：语料料库是我们要分析的所有文档的集合
2.中文分词：指的是将一个汉字序列切成一个一个单独的词
3.停 用 词：数据处理的时候，自动过滤掉某些字或词，如：web，网站等

##  什么是词云

1. 统计文本中的高频词
2. 过滤掉某些常用词（比如“作词”“作曲”）
3. 将重要关键词可视化，便于了解文本重点，
4. 具有一定的美观度

## 工具

python ： WordCloud

中文需要先分词并组成空格分隔字符串。

参考

1. CSDN：wordcloud基本使用关键步骤 https://blog.csdn.net/qq_36206070/article/details/106414365

2. 知乎：Python词云 wordcloud 十五分钟入门与进阶 https://zhuanlan.zhihu.com/p/27626809 
3. CSDN：【上中课程】词云Word Cloud（标签云、词频分析、文本分析）的实现——应用jieba库和wordcloud库https://blog.csdn.net/ling888666/article/details/88641789?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link

# WordCloud

## 使用说明

wordcloud库把词云当作一个WordCloud对象

- wordcloud.WordCloud()代表一个文本对应的词云
- 可以根据文本中词语出现的频率等参数绘制词云
- 绘制词云的形状、尺寸和颜色都可以设定

## 安装

`conda install -c conda-forge wordcloud`

## 常规使用方法

step1:创建一个词云对象WordCloud

Step2:配置词云对象的参数

Step3:加载词云文本

- `w.generate(txt)`	

- 向WordCloud对象w中加载文本txt，w.generate("Python and WordCloud")

Step4: 生成词云文件

- `w.to_file(filename)`	

- 将词云输出为图像文件，.png或.jpg，w.to_file("outfile.png")

## 参数对象

配置参数对象：`w = word cloud.WordCloud(<参数>)`

|       参数       |                       描述                       |
| :--------------: | :----------------------------------------------: |
|      width       |     指定词云对象生成图片的宽度，默认400像素      |
|      height      |     指定词云对象生成图片的高度，默认200像素      |
|  min_font_size   |        指定词云中字体的最小字号，默认4号         |
|  max_font_size   |    指定词云中字体的最大字号，根据高度自动调节    |
|    font_step     |      指定词云中字体字号的步进间隔，默认为1       |
|    font_path     |           指定字体文件的路径，默认None           |
|    max_words     |       指定词云显示的最大单词数量，默认200        |
|    stop_words    |     指定词云的排除词列表，即不显示的单词列表     |
|       mask       | 指定词云形状，默认为长方形，需要引用imread()函数 |
| background_color |        指定词云图片的背景颜色，默认为黑色        |

## 示例

```python
import wordcloud

w = wordcloud.WordCloud()
w.generate("Python and WordCloud")
w.to_file("pywordcloud.png")
```

# 结巴分词

知乎：jieba分词-强大的Python 中文分词库https://zhuanlan.zhihu.com/p/207057233?utm_source=wechat_timeline

jieba最靠谱的文档是github项目的readme，因为它似乎还没有独立的使用文档。但由于使用起来简单，看readme也能快速上手。*[https://github.com/fxsjy/jieba](https://link.zhihu.com/?target=https%3A//github.com/fxsjy/jieba)*

## 安装

支持pip或conda安装

`conda install jieba`

## 分词初体验

所谓分词就是将一段表述里的词汇进行分解，比如“我爱中国”，分解后有三个词：我、爱、中国，词性分别是名词、动词、名词。

jieba库中用于分词的方法有三个:

### **jieba.cut**

给定中文字符串，分解后返回一个**迭代器**，需要用for循环访问。

参数解释：

**「strs」**： 需要分词的字符串；待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8

**「cut_all」**：用来控制是否采用全模式；
**「HMM」**：用来控制是否使用 HMM 模型；
**「use_paddle」**：用来控制是否使用paddle模式下的分词模式，paddle模式采用延迟加载方式，通过enable_paddle接口安装paddlepaddle-tiny，并且import相关代码；

> HMM模型：**隐马尔可夫模型**（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。
>
> 其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。
>
> 现已成功地用于语音识别，行为识别，文字识别以及故障诊断等领域。
>
> https://baike.baidu.com/item/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/7932524?fr=aladdin

> viterbi维特比算法：解决的是篱笆型的图的最短路径问题————图的节点按列组织，每列的节点数量可以不一样，每一列的节点只能和相邻列的节点相连，不能跨列相连，节点之间有着不同的距离
>
> https://www.zhihu.com/question/20136144

这里区分全模式和精确模式，举个例子先看看区别：

```python3
# 全模式
seg_list = jieba.cut("中国上海是一座美丽的国际性大都市", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  

# 返回结果
Full Mode: 中国/ 上海/ 是/ 一座/ 美丽/ 的/ 国际/ 国际性/ 大都/ 大都市/ 都市

# 精确模式
seg_list = jieba.cut("中国上海是一座美丽的国际性大都市", cut_all=False)
print("Full Mode: " + "/ ".join(seg_list))  

# 返回结果
Default Mode: 中国/ 上海/ 是/ 一座/ 美丽/ 的/ 国际性/ 大都市
```

可以看到，全模式把句子中所有的可以成词的词语都扫描出来, 会出现一词多用、一词多意。精确模式将句子最精确的切分开，每个词都只有一种含义。

`jieba.cut`方法默认是精确模式。

还有一个参数控制paddle模式，会更加精确，使用这个的前提是你需要先安装paddlepaddle-tiny。

安装命令：
`pip install paddlepaddle-tiny==1.6.1`

详情可以去官网看下，这里不举例。

### **jieba.cut_for_search**

该方法和cut一样，分解后返回一个迭代器，需要用for循环访问。不过它是搜索引擎模式，在**精确模式的基础上，对长词再次切分，提高召回率**，适合用于搜索引擎分词。

参数解释：

**「strs」**：需要分词的字符串；
**「HMM」**：是否使用 HMM 模型，默认值为 True。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细。

```python3
# 搜索引擎模式
seg_list = jieba.cut_for_search("中国上海是一座美丽的国际性大都市，拥有复旦大学、上海交通大学等知名高等学府")  
print(", ".join(seg_list))

# 返回结果
Search Mode: 中国, 上海, 是, 一座, 美丽, 的, 国际, 国际性, 大都, 都市, 大都市, ，, 拥有, 复旦, 大学, 复旦大学, 、, 上海, 交通, 大学, 上海交通大学, 等, 知名, 高等, 学府, 高等学府
```

### **jieba.lcut**， jieba.lcut_for_search

- `jieba.lcut` 以及 `jieba.lcut_for_search` 直接返回 list

cut和cut_for_search方法都是支持繁体字的。

## 添加自定义词典

如果是对专业新闻或者小说进行分词，会有很多的新词汇，jieba库里没有就没办法识别，那么就需要添加自定义的词汇，比如：奥利给。

添加自定义词汇的方法： `jieba.load_userdict(file_name)` 参数是文本文件，txt、csv都可以。

自定义词典文件的词汇格式是一个词占一行，每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。 比如：

![img](数据可视化——歌词词云展示.assets/v2-006e9b57a6f284ddce4f14314b8caa7a_1440w.jpg)

以"奥利给，管虎执导的八佰是一部让人热血沸腾的好电影。" 这段话为例， 如果不添加自定义词典，很多词没办法识别出来。

```python3
# 不添加自定义词典
seg_list = jieba.cut("奥利给，管虎执导的八佰是一部让人热血沸腾的好电影")
print("/ ".join(seg_list))  

# 返回结果
奥利/ 给/ ，/ 管虎/ 执导/ 的/ 八佰是/ 一部/ 让/ 人/ 热血沸腾/ 的/ 好/ 电影
```

添加自定义词典后，新词、人名、电影名都可以识别出来

```python3
# 载入词典
jieba.load_userdict("dict.txt")
seg_list = jieba.cut("奥利给，管虎执导的八佰是一部让人热血沸腾的好电影")
print("/ ".join(seg_list))  

# 返回结果
奥利给/ ，/ 管虎/ 执导/ 的/ 八佰/ 是/ 一部/ 让/ 人/ 热血沸腾/ 的/ 好/ 电影
```

## 其他功能

前面讲了一些基本的使用，大家还可以尝试使用停用词、提取关键词、词性标注、词位置查询等功能，也是十分的便捷。

# PIL vs Pillow

链接：简书 https://www.jianshu.com/p/996a6032254c

## PIL 

`PIL`全称是：Python Imaging Library。

`PIL`是一个强大的、方便的python图像处理库，功能非常强大，曾经一度被认为是python平台事实上的图像处理标准库，不过**Python 2.7以后不再支持**。

`PIL`官方网站：

```ruby
http://pythonware.com/products/pil/
```

该软件包提供了基本的图像处理功能，如：改变图像大小，旋转图像，图像格式转换，色场空间转换，图像增强，直方图处理，插值和滤波等等。

虽然在这个软件包上要实现类似MATLAB中的复杂的图像处理算法并不太适合，但是Python的快速开发能力以及面向对象等等诸多特点使得它非常适合用来进行原型开发。

在PIL中，任何一副图像都是用一个Image对象表示，而这个类由和它同名的模块导出，因此，要加载一副图像，最简单的形式是这样的：

```
import Image
img = Image.open(“dip.jpg”)
```

注意：第一行的Image是模块名；第二行的img是一个Image对象； Image类是在Image模块中定义的。关于Image模块和Image类，切记不要混淆了。现在，我们就可以对img进行各种操作了，所有对img的 操作最终都会反映到到dip.img图像上。

## Pillow

`Pillow`是基于`PIL`模块fork的一个派生分支，但如今已经发展成为比`PIL`本身更具活力的图像处理库。

`Pillow`友好支持python3。

`Pillow`官方文档地址：

```ruby
https://pillow.readthedocs.io/en/stable/
```

> `Pillow`和`PIL`不能在同一个环境中共存。在安装`Pillow`之前，请先卸载`PIL`。

简单使用`Pillow`：

```python
# 从Pillow导入Image模块
from PIL import Image

# 打开图片bride.jpg
im = Image.open("bride.jpg")

# 显示图片
im.rotate(45).show()
```

> 正是这里的`from PIL`，可能会让很多新人疑惑。虽然是`pillow`，但是导入包的写法依然是`from PIL`。

> ```
> Image`只是`pillow`众多模块之一。其他模块以及对应文档参照官网：`https://pillow.readthedocs.io/en/stable/reference/index.html
> ```

到此，`PIL`和`Pillow`介绍完毕！



# 字体

在 Mac 上的“字体册”中导出字体：

*【注】*导出字体前，请先检查字体的许可协议：在工具栏中点按字体的“信息”按钮 ，然后向下滚动到许可信息。

1. 在 Mac 上的“字体册” App  中，选择一种或多种字体或字体系列，或者选择一个字体集以导出。
2. 选取“文件”>“导出字体”，选取字体的存储位置，然后点按“存储”。



